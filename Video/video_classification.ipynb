{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run activity recognition on video data using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = foz.load_zoo_dataset(\n",
    "    \"activitynet-200\",\n",
    "    split=\"validation\",\n",
    "    max_duration=30,\n",
    "    max_samples=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(dataset, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate frame objects for each frame in each of our videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metadata...\n",
      " 100% |█████████████████| 100/100 [2.3s elapsed, 0s remaining, 44.0 samples/s]  \n"
     ]
    }
   ],
   "source": [
    "dataset.ensure_frames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temporary image files for frames in our video samples. They will be stored in the `/tmp` directory. We pass `size=(224, 224)` to resize the frames to 224x224 pixels for compatibility with the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 9172/9172 [142.1ms elapsed, 0s remaining, 65.1K samples/s]  \n",
      "Sampling video frames...\n",
      " 100% |█████████████████| 100/100 [23.9s elapsed, 0s remaining, 5.0 samples/s]      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset:     activitynet-200-validation-100\n",
       "Media type:  image\n",
       "Num samples: 9172\n",
       "Sample fields:\n",
       "    id:           fiftyone.core.fields.ObjectIdField\n",
       "    sample_id:    fiftyone.core.fields.ObjectIdField\n",
       "    filepath:     fiftyone.core.fields.StringField\n",
       "    frame_number: fiftyone.core.fields.FrameNumberField\n",
       "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "View stages:\n",
       "    1. ToFrames(config={'force_sample': True, 'max_fps': 5, 'output_dir': '/tmp/', ...})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_frames(sample_frames=True, max_fps=5, output_dir=\"/tmp/\", force_sample=True, size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create a list of frame arrays for a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def _construct_frames_array(frame_fps):\n",
    "    frames = []\n",
    "    num_frames = 0\n",
    "    for frame_fp in frame_fps:\n",
    "        if frame_fp is None:\n",
    "            continue\n",
    "        \n",
    "        image = Image.open(frame_fp)\n",
    "        image = np.transpose(np.array(image), (2, 0, 1))\n",
    "        frames.append(image)\n",
    "        num_frames += 1\n",
    "\n",
    "        if num_frames == 16:\n",
    "            return frames\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and run our pretrained model across all samples in the dataset, adding the predictions to a new field of type `fo.Classification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
    "\n",
    "frame_fp_lists = dataset.values(\"frames.filepath\")\n",
    "\n",
    "predicted_labels = []\n",
    "\n",
    "for frame_fps in tqdm(frame_fp_lists):\n",
    "    video = _construct_frames_array(frame_fps)\n",
    "    inputs = processor(video, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    predicted_class = model.config.id2label[predicted_class_idx]\n",
    "    label = fo.Classification(label=predicted_class)\n",
    "    predicted_labels.append(label)\n",
    "\n",
    "\n",
    "dataset.set_values(\"predicted_labels\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Video Classification](../assets/video_classification.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
