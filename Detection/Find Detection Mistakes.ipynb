{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a291c3",
   "metadata": {},
   "source": [
    "# Find Detection Mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c4de5",
   "metadata": {},
   "source": [
    "Annotations mistakes create an artificial ceiling on the performance of your models. However, finding these mistakes by hand is at least as arduous as the original annotation work! Enter FiftyOne.\n",
    "\n",
    "In this tutorial, we explore how FiftyOne can be used to help you find mistakes in your object detection annotations. To detect mistakes in classification datasets, check out the recipe in the Classification task.\n",
    "\n",
    "We’ll cover the following concepts:\n",
    "\n",
    "- Computing insights into your dataset relating to possible label mistakes\n",
    "- Visualizing mistakes in the FiftyOne App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbfbb5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e618d",
   "metadata": {},
   "source": [
    "If you haven't already, install FiftyOne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97634157",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e1315",
   "metadata": {},
   "source": [
    "In order to compute mistakenness, your dataset needs to have two [detections fields](https://docs.voxel51.com/user_guide/using_datasets.html#object-detection), one with your ground truth annotations and one with your model predictions.\n",
    "\n",
    "In this example, we’ll load the [quickstart](https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html#dataset-zoo-quickstart) dataset from the FiftyOne Dataset Zoo, which has ground truth annotations and predictions from a PyTorch Faster-RCNN model for a few samples from the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa9a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |█████████████████| 200/200 [8.6s elapsed, 0s remaining, 17.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "results = dataset.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"eval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eaec70",
   "metadata": {},
   "source": [
    "We can start too by visualizing our dataset. See any mistakes yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a875861",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7553cd5",
   "metadata": {},
   "source": [
    "## Compute Mistakenness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f04e0",
   "metadata": {},
   "source": [
    "Now we’re ready to assess the mistakenness of the ground truth detections.\n",
    "\n",
    "We can do so by running the [compute_mistakenness()](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_mistakenness) method from the FiftyOne Brain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9419bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |█████████████████| 200/200 [8.5s elapsed, 0s remaining, 16.8 samples/s]      \n",
      "Computing mistakenness...\n",
      " 100% |█████████████████| 200/200 [1.6s elapsed, 0s remaining, 113.5 samples/s]         \n",
      "Mistakenness computation complete\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "# Compute mistakenness of annotations in `ground_truth` field using\n",
    "# predictions from `predictions` field as point of reference\n",
    "fob.compute_mistakenness(dataset, \"predictions\", label_field=\"ground_truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e0768",
   "metadata": {},
   "source": [
    "The above method populates a number of fields on the samples of our dataset as well as the ground truth and predicted objects:\n",
    "\n",
    "New ground truth object attributes (in `ground_truth` field):\n",
    "\n",
    "- `mistakenness` (float): A measure of the likelihood that a ground truth object’s label is incorrect\n",
    "- `mistakenness_loc`: A measure of the likelihood that a ground truth object’s localization (bounding box) is inaccurate\n",
    "- `possible_spurious`: Ground truth objects that were not matched with a predicted object and are deemed to be likely spurious annotations will have this attribute set to True\n",
    "\n",
    "New predicted object attributes (in `predictions` field):\n",
    "\n",
    "- `possible_missing`: If a highly confident prediction with no matching ground truth object is encountered, this attribute is set to True to indicate that it is a likely missing ground truth annotation\n",
    "\n",
    "Sample-level fields:\n",
    "\n",
    "- `mistakenness`: The maximum mistakenness of the ground truth objects in each sample\n",
    "- `possible_spurious`: The number of possible spurious ground truth objects in each sample\n",
    "- `possible_missing`: The number of possible missing ground truth objects in each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662a2aa",
   "metadata": {},
   "source": [
    "## Analyzing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accee12",
   "metadata": {},
   "source": [
    "Let’s use FiftyOne to investigate the results.\n",
    "\n",
    "First, let’s show the samples with the most likely annotation mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c1dde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:     quickstart\n",
      "Media type:  image\n",
      "Num samples: 200\n",
      "Sample fields:\n",
      "    id:                fiftyone.core.fields.ObjectIdField\n",
      "    filepath:          fiftyone.core.fields.StringField\n",
      "    tags:              fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    ground_truth:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    uniqueness:        fiftyone.core.fields.FloatField\n",
      "    predictions:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    mistakenness:      fiftyone.core.fields.FloatField\n",
      "    possible_missing:  fiftyone.core.fields.IntField\n",
      "    possible_spurious: fiftyone.core.fields.IntField\n",
      "View stages:\n",
      "    1. SortBy(field_or_expr='mistakenness', reverse=True, create_index=True)\n"
     ]
    }
   ],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Sort by likelihood of mistake (most likely first)\n",
    "mistake_view = dataset.sort_by(\"mistakenness\", reverse=True)\n",
    "\n",
    "# Print some information about the view\n",
    "print(mistake_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0361d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Detection: {\n",
      "    'id': '5f452487ef00e6374aad2744',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'tv',\n",
      "    'bounding_box': [\n",
      "        0.002746666666666667,\n",
      "        0.36082,\n",
      "        0.24466666666666667,\n",
      "        0.3732,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': None,\n",
      "    'index': None,\n",
      "    'area': 16273.3536,\n",
      "    'iscrowd': 0.0,\n",
      "    'mistakenness': 0.005771428346633911,\n",
      "    'mistakenness_loc': 0.16955941131917984,\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "# Inspect some samples and detections\n",
    "# This is the first detection of the first sample\n",
    "print(mistake_view.first().ground_truth.detections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f55539",
   "metadata": {},
   "source": [
    "Let’s use the App to visually inspect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f657743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the samples we processed in rank order by the mistakenness\n",
    "session.view = mistake_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53d805",
   "metadata": {},
   "source": [
    "![mistakeness](../assets/mistakeness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eacf7f",
   "metadata": {},
   "source": [
    "Another useful query is to find all objects that have a high mistakenness, lets say > 0.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396636ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "session.view = dataset.filter_labels(\"ground_truth\", F(\"mistakenness\") > 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e0675",
   "metadata": {},
   "source": [
    "![mistake_view](../assets/mistake_view.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be1f75",
   "metadata": {},
   "source": [
    "Looking through the results, we can see that many of these images have a bunch of predictions which actually look like they are correct, but no ground truth annotations. This is a common mistake in object detection datasets, where the annotator may have missed some objects in the image. On the other hand, there are some detections which are mislabeled, like the `cow` in the last image above which is predicted to be a `horse`.\n",
    "\n",
    "We can use a similar workflow to look at objects that may be localized poorly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dataset.filter_labels(\"ground_truth\", F(\"mistakenness_loc\") > 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9b485",
   "metadata": {},
   "source": [
    "![mistake_loc](../assets/mistake_loc.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79961e",
   "metadata": {},
   "source": [
    "In some of these examples, there is not necessarily highly mistaken localization, there are just a bunch of small, relatively overlapping objects. In other examples, such as above, the localization is clearly off.\n",
    "\n",
    "The `possible_missing` field can also be useful to sort by to find instances of incorrect annotations.\n",
    "\n",
    "Similarly, `possible_spurious` can be used to find objects that the model detected that may have been missed by annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326004f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dataset.match(F(\"possible_missing\") > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847e6a5",
   "metadata": {},
   "source": [
    "![mistake_missing](../assets/mistake_missing.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95706a93",
   "metadata": {},
   "source": [
    "Once again, we can find more of those pesky mistakes! In FiftyOne, we can tag our samples and export them for annotation job with one of labeling integrations: [CVAT](https://docs.voxel51.com/integrations/cvat.html), [Label Studio](https://docs.voxel51.com/integrations/labelstudio.html), [V7](https://docs.voxel51.com/integrations/v7.html), or [LabelBox](https://docs.voxel51.com/integrations/labelbox.html)! This can get our dataset back into tip-top shape to train again!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
