{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6e3fa8-681c-40d7-b694-e98c02138174",
   "metadata": {},
   "source": [
    "# Open Vocabuary Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11277f83-26bc-4dc9-90f6-b1c9a08eef9e",
   "metadata": {},
   "source": [
    "World models or Open-Vocabulary Models are an innovation that enables the detection of any object within an image based on descriptive texts. The YOLO-World Model introduced an advanced, real-time Ultralytics YOLOv8-based approach for Open-Vocabulary Detection tasks, while still maintaining a significantly low computational demand.\n",
    "\n",
    "In this recipe, we will demonstrate how to apply the [Ultralytics YOLO-World Model](https://docs.ultralytics.com/models/yolo-world/) directly to your FiftyOne dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f2b00-4203-4bee-b5fd-d08f32cac043",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912294a-5b6d-4bdb-a162-0b0a50a036db",
   "metadata": {},
   "source": [
    "If you havent already, install FiftyOne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0725a-41fc-4911-b3ab-6c4c38d506c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821fd22b-15ae-4bb8-a267-d48dcb4a8d94",
   "metadata": {},
   "source": [
    "We will also need to install some prerequisite libraries to run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b37b36-556a-45ce-bdc6-196d4090b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf96495-5336-4bea-8d59-6f6e9fe78b1e",
   "metadata": {},
   "source": [
    "In this example, weâ€™ll load the [quickstart](https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html#dataset-zoo-quickstart) dataset from the FiftyOne Dataset Zoo, which has ground truth annotations and predictions from a PyTorch Faster-RCNN model for a few samples from the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98435271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6df61-037c-4e3d-af81-77eddf9be23e",
   "metadata": {},
   "source": [
    "After loading in our app, we can apply the YOLOWorld model using `apply_model`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOWorld\n",
    "\n",
    "# Initialize a YOLO-World model\n",
    "model = YOLOWorld('yolov8s-world.pt')  # or select yolov8m/l-world.pt for different sizes\n",
    "dataset.apply_model(model, label_field=\"YOLOWorld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6c3b1-d484-4a94-92fa-d083a8795d0c",
   "metadata": {},
   "source": [
    "![YOLO-WORLD](../assets/world_model.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3b155-b144-4dfa-9677-0b022a3d791c",
   "metadata": {},
   "source": [
    "Let's see all the different labels our world model predicted with open vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe76d789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'apple',\n",
       " 'backpack',\n",
       " 'banana',\n",
       " 'baseball bat',\n",
       " 'baseball glove',\n",
       " 'bear',\n",
       " 'bed',\n",
       " 'bench',\n",
       " 'bird',\n",
       " 'boat',\n",
       " 'book',\n",
       " 'bottle',\n",
       " 'bowl',\n",
       " 'broccoli',\n",
       " 'bus',\n",
       " 'cake',\n",
       " 'car',\n",
       " 'carrot',\n",
       " 'cat',\n",
       " 'cell phone',\n",
       " 'chair',\n",
       " 'clock',\n",
       " 'couch',\n",
       " 'cow',\n",
       " 'cup',\n",
       " 'dining table',\n",
       " 'dog',\n",
       " 'donut',\n",
       " 'elephant',\n",
       " 'fire hydrant',\n",
       " 'fork',\n",
       " 'frisbee',\n",
       " 'giraffe',\n",
       " 'handbag',\n",
       " 'horse',\n",
       " 'hot dog',\n",
       " 'kite',\n",
       " 'knife',\n",
       " 'laptop',\n",
       " 'microwave',\n",
       " 'motorcycle',\n",
       " 'mouse',\n",
       " 'orange',\n",
       " 'oven',\n",
       " 'person',\n",
       " 'pizza',\n",
       " 'refrigerator',\n",
       " 'sandwich',\n",
       " 'sheep',\n",
       " 'sink',\n",
       " 'skateboard',\n",
       " 'skis',\n",
       " 'snowboard',\n",
       " 'spoon',\n",
       " 'sports ball',\n",
       " 'stop sign',\n",
       " 'suitcase',\n",
       " 'surfboard',\n",
       " 'teddy bear',\n",
       " 'tennis racket',\n",
       " 'tie',\n",
       " 'toaster',\n",
       " 'toilet',\n",
       " 'toothbrush',\n",
       " 'traffic light',\n",
       " 'train',\n",
       " 'truck',\n",
       " 'tv',\n",
       " 'umbrella',\n",
       " 'vase',\n",
       " 'wine glass',\n",
       " 'zebra']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.distinct(\"YOLOWorld.detections.label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
