{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608dc5d2",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6fd55",
   "metadata": {},
   "source": [
    "Traditionally, computer vision models are trained to predict a fixed set of categories. For image classification, for instance, many standard models are trained on the ImageNet dataset, which contains 1,000 categories. All images must be assigned to one of these 1,000 categories, and the model is trained to predict the correct category for each image.\n",
    "\n",
    "For object detection, many popular models like YOLOv5, YOLOv8, and YOLO-NAS are trained on the MS COCO dataset, which contains 80 categories. In other words, the model is trained to detect objects in any of these categories, and ignore all other objects.\n",
    "\n",
    "Thanks to the recent advances in multimodal models, it is now possible to perform zero-shot learning, which allows us to predict categories that were not seen during training. This can be especially useful when:\n",
    "\n",
    "- We want to roughly pre-label images with a new set of categories\n",
    "- Obtaining labeled data for all categories is impractical or impossible.\n",
    "- The categories are changing over time, and we want to predict new categories without retraining the model.\n",
    "\n",
    "In this recipe, we will show how you can quickly add zero-shot predictions to your dataset. Check [here](https://docs.voxel51.com/tutorials/zero_shot_classification.html#Evaluating-Zero-Shot-Image-Classification-Predictions-with-FiftyOne) for a more in depth tutorial on zero-shot image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd6917",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6712c",
   "metadata": {},
   "source": [
    "If you haven't already, install FiftyOne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893deaf5",
   "metadata": {},
   "source": [
    "We will also need the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch torchvision fiftyone transformers timm open_clip_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179e4f5",
   "metadata": {},
   "source": [
    "Now let’s import the relevant modules and load the dataset!\n",
    "\n",
    "For this walkthrough, we will use the [Caltech-256 dataset](https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html#caltech-256), which contains 30,607 images across 257 categories. We will use 1000 randomly selected images from the dataset for demonstration purposes. The zero-shot models were not explicitly trained on the Caltech-256 dataset, so we will use this as a test of the models’ zero-shot capabilities. Of course, you can use any dataset you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d039bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"caltech256\",\n",
    "    max_samples=1000,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset.name = \"Zero-Shot Classification\"\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2eef5e",
   "metadata": {},
   "source": [
    "![zero-shot](../assets/zero-shot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d607d",
   "metadata": {},
   "source": [
    "First, we need to start by grabbing the classes that we will want our zero-shot model to use. In our case, let us grab the ground_truth labels of Caltech-256 with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "857c71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dataset.distinct(\"ground_truth.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b631336",
   "metadata": {},
   "source": [
    "## Zero-Shot Image Classification with OpenAI CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf935b09",
   "metadata": {},
   "source": [
    "We can start off with the natively supported Open AI CLIP model, we can load and apply it to our dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9210bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1000/1000 [4.5s elapsed, 0s remaining, 305.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "clip = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "dataset.apply_model(clip, label_field=\"clip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f71e3e",
   "metadata": {},
   "source": [
    "We can take a look at our new results right away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f0d6f",
   "metadata": {},
   "source": [
    "![OpenAi CLIP](../assets/openai-clip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d488c2",
   "metadata": {},
   "source": [
    "Want to try a different model? We have tons to choose from including any from [Hugging Face](https://docs.voxel51.com/tutorials/zero_shot_classification.html#Zero-Shot-Image-Classification-with-Hugging-Face-Transformers) as well as [OpenClip](https://docs.voxel51.com/tutorials/zero_shot_classification.html#Zero-Shot-Image-Classification-with-OpenCLIP)! Here we load AltCLIP from HuggingFace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac332fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/envs/dev/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/dan/anaconda3/envs/dev/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||--------------|    0/1000 [3.4ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/envs/dev/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1000/1000 [13.5m elapsed, 0s remaining, 1.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "model = foz.load_zoo_model(\n",
    "        \"zero-shot-classification-transformer-torch\",\n",
    "        name_or_path=\"kakaobrain/align-base\",\n",
    "        classes=classes,\n",
    "    )\n",
    "\n",
    "dataset.apply_model(model, label_field=\"AltCLIP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7d6aff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=ef916657-4257-4064-b63b-905bf7105698\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x72acc460a640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
