{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48816a2",
   "metadata": {},
   "source": [
    "# 3D Reconstruction Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5700c98",
   "metadata": {},
   "source": [
    "One of the key computer vision problems today is to be able to create 3D models or spaces based on a 2D input. In this tutorial, we will look at how we can use FiftyOne and 3D Reconstruction models to generate 3D meshes. Let's start with just a single image with Single Shot Reconstruction!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac4f76",
   "metadata": {},
   "source": [
    "## Single Shot Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ddb87",
   "metadata": {},
   "source": [
    "3D Reconstruction can be performed with a variety of different inputs, but what makes Single Shot Reconstruction special is that it only requires a single input image. The hierarchy of models will then generate all the necesary components to generate a 3D mesh with textures. We will be checking out [Instant Mesh](https://github.com/TencentARC/InstantMesh) to generate a 3D model of our favorite Pokemon! We can kick things off by downloading our [Pokemon dataset](https://www.kaggle.com/datasets/rounakbanik/pokemon) from kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce7673-f1f0-4051-932c-a19cdce37fd1",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d divyanshusingh369/complete-pokemon-library-32k-images-and-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip complete-pokemon-library-32k-images-and-csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f477e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TencentARC/InstantMesh.git && cd InstantMesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d43e9d",
   "metadata": {},
   "source": [
    "Instant Mesh Install Steps for bash ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef85c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name instantmesh python=3.10\n",
    "conda activate instantmesh\n",
    "pip install -U pip\n",
    "\n",
    "# Ensure Ninja is installed\n",
    "conda install Ninja\n",
    "\n",
    "# Install the correct version of CUDA\n",
    "conda install cuda -c nvidia/label/cuda-12.1.0\n",
    "\n",
    "# Install PyTorch and xformers\n",
    "# You may need to install another xformers version if you use a different PyTorch version\n",
    "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install xformers==0.0.22.post7\n",
    "\n",
    "# For Linux users: Install Triton \n",
    "pip install triton\n",
    "\n",
    "# For Windows users: Use the prebuilt version of Triton provided here:\n",
    "pip install https://huggingface.co/r4ziel/xformers_pre_built/resolve/main/triton-2.0.0-cp310-cp310-win_amd64.whl\n",
    "\n",
    "# Install other requirements\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfc2a7-aaba-4b7b-b75a-1bf1ee84ba3c",
   "metadata": {},
   "source": [
    "### Loading Our Pokemon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e70a5",
   "metadata": {},
   "source": [
    "We load our dataset in seemlessly using the [ImageClassificationDirectoryTree](https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#imageclassificationdirectorytree-import) importer. After the dataset is in, we can visualize it by launching the app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c51d0c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2374/2374 [482.7ms elapsed, 0s remaining, 4.9K samples/s]      \n",
      "Name:        Pokemon Images\n",
      "Media type:  image\n",
      "Num samples: 2374\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:           fiftyone.core.fields.ObjectIdField\n",
      "    filepath:     fiftyone.core.fields.StringField\n",
      "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import os\n",
    "\n",
    "name = \"Pokemon Images\"\n",
    "dataset_dir = \"Pokemon Images DB/Pokemon Images DB/\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
    "    name=name,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "dataset.persistent = True\n",
    "\n",
    "# View summary info about the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39306335-a1f4-470b-8fb0-3fb50c20ed3a",
   "metadata": {},
   "source": [
    "We can launch the app to take a look at all of our Pokemon with both a white and black backround to play around with. The flexible background is great as for some image to 3D examples, one background type may work much better than others!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ca326",
   "metadata": {},
   "source": [
    "Generating meshes can be very computationally expensive, so in order to save time, let us choose one or two of our favorite Pokemon. Don't know which to pick? Explore the app and find one that you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79340a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "view = dataset.filter_labels(\n",
    "    \"ground_truth\", (F(\"label\") == \"Pikachu\") | (F(\"label\") == \"Charmander\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11412943",
   "metadata": {},
   "source": [
    "Instant Mesh provides tons of information along the way of generating our mesh that we are able to visualize in the app. Since we are going to have samples with images, videos, and meshes, this is a perfect oppurtunity for us to create a new [FiftyOne Grouped Dataset](https://docs.voxel51.com/user_guide/groups.html), a dataset that can hold groups of samples of any modality you like! We start by creating the daaset and adding a group slice to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1229237",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Pokemon Meshes\"\n",
    "# Create the dataset\n",
    "mesh_dataset = fo.Dataset(name,overwrite=True)\n",
    "\n",
    "mesh_dataset.add_group_field(\"group\", default=\"Pokemon Picture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4bffcb-2921-465e-a319-131cebad7db3",
   "metadata": {},
   "source": [
    "### Generating Our New Single Shot Meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa6104",
   "metadata": {},
   "source": [
    "The tutorial provides two different ways to generate the mesh. If you are able to run the GPU intensive model Instant Mesh, the following below is an example on how. If not, there is also an option to run Instant Mesh on [Replicate](https://replicate.com/camenduru/instantmesh/api), an easy to use model serving provider. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101196f3-8487-4275-94e0-5866a76c5ba1",
   "metadata": {},
   "source": [
    "### Local InstantMesh Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09a7dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meshes(sample, output_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    A function that runs InstantMesh locally using python subprocess. InstantMesh will then save all generated files to the output_dir.\n",
    "\n",
    "    Args:\n",
    "        sample: A FiftyOne sample to pass into InstantMesh, must have media_type = \"image\"\n",
    "        output_dir: The destination of generated files by InstantMesh\n",
    "    \"\"\"\n",
    "    \n",
    "    # Grab the filepath to pass in\n",
    "    name =  sample.filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    filepath = os.path.abspath(sample.filepath)\n",
    "    filepath = '\"' + filepath + '\"'\n",
    "    \n",
    "    command = \"cd ./InstantMesh && python run.py configs/instant-mesh-base.yaml \" + \\\n",
    "    filepath + \" --export_texmap --output_path \" + output_dir\n",
    "    \n",
    "    # Run InstantMesh through subprocess\n",
    "    result = subprocess.run(command, shell=True,capture_output=True, text=True)\n",
    "    # uncomment to see model progress!\n",
    "    # print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be159cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grouped_mesh_samples(sample, output_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    A function that loads the InstantMesh outputs into a FiftyOne Group. Returns a list of all the samples in the group.\n",
    "\n",
    "    Args:\n",
    "        sample: The original sample used to generate the Instant Mesh outputs\n",
    "        output_dir: The destination of generated files by InstantMesh\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    # Define Our Group\n",
    "    group = fo.Group()\n",
    "\n",
    "    name =  sample.filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    # First we add the input Pokemon picture\n",
    "    input_sample = fo.Sample(filepath=sample.filepath, group=group.element(\"Pokemon Picture\"))\n",
    "    samples.append(input_sample)\n",
    "    \n",
    "    # Next we add the intermediary stereo image\n",
    "    mv_show_path = output_dir + \"/instant-mesh-base/images/\" + name + \".png\"\n",
    "    stereo_sample = fo.Sample(filepath=mv_show_path, group=group.element(\"MV Show Image\"))\n",
    "    samples.append(stereo_sample)\n",
    "    \n",
    "    \n",
    "    # Next we add a video of our 3D mesh if exists\n",
    "    video_path = output_dir + \"/instant-mesh-base/videos/\" + name + \".mp4\"\n",
    "    if os.path.exists(video_path):\n",
    "        video_sample = fo.Sample(filepath=video_path, group=group.element(\"Video\"))\n",
    "        samples.append(video_sample)\n",
    "    \n",
    "    # Next we define the paths to our mesh files\n",
    "    obj_path = output_dir + \"/instant-mesh-base/meshes/\"  + name + \".obj\"\n",
    "    \n",
    "    mtl_path = output_dir + \"/instant-mesh-base/meshes/\" + name + \".mtl\"\n",
    "\n",
    "    # We load in with ObjMesh and pass our files in\n",
    "    obj = fo.ObjMesh(name=sample[\"ground_truth\"].label, obj_path=obj_path, mtl_path=mtl_path)\n",
    "    \n",
    "    # We define our scene and save it so it can be loaded into FiftyOne\n",
    "    scene = fo.Scene(camera=fo.PerspectiveCamera(up=\"Z\"))\n",
    "    scene.add(obj)\n",
    "    scene_path = output_dir + \"/instant-mesh-base/meshes/\" + sample.id + \".fo3d\"\n",
    "    scene.write(scene_path)\n",
    "        \n",
    "    # Finally we add the scene as a sample\n",
    "    mesh_sample = fo.Sample(scene_path, group=group.element(\"Mesh\"))\n",
    "    samples.append(mesh_sample)\n",
    "    \n",
    "    # Return all our new samples back\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a443d-a70b-4b44-88fb-a4bf10d6327c",
   "metadata": {},
   "source": [
    "With our helper functions defined, we can now run Instant Mesh and view our newly generated meshes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5aae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "output_dir = \"/path/to/output_dir\"\n",
    "\n",
    "\n",
    "for poke_sample in view:\n",
    "    \n",
    "    generate_meshes(poke_sample, output_dir)\n",
    "    samples = create_grouped_mesh_samples(poke_sample, output_dir)\n",
    "    \n",
    "    mesh_dataset.add_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a20934-b28d-445b-9414-265aa6f785a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session.dataset = mesh_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c9bb2-899f-41ac-a313-f2864abf1dec",
   "metadata": {},
   "source": [
    "Let's see how our Pikachu turned out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3342863-7eb8-4006-8afb-3751a1f49f33",
   "metadata": {},
   "source": [
    "![pikachu](../assets/pikachu.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0c26a-20c5-4a77-b1a6-4b0cc1ce2807",
   "metadata": {},
   "source": [
    "### Run with Replicate Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e563df0-8ff3-43fa-a2e9-d9ebd2bcb63b",
   "metadata": {},
   "source": [
    "Below we can use [replicate](https://replicate.com) to serve us [InstantMesh](https://replicate.com/camenduru/instantmesh) as well! Define your $REPLICATE_API_TOKEN before running in the terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c21ed-65e2-4a85-a9d0-1d8e8cc3c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88396f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_image(image_url, filename):\n",
    "    img_data = requests.get(image_url).content\n",
    "    with open(filename, \"wb\") as handler:\n",
    "        handler.write(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809be57-0074-4378-b843-a9649ad9abe6",
   "metadata": {},
   "source": [
    "Let us create some destinations for the new files we will be creating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26d736b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/path/to/instantmesh_outputs\"\n",
    "\n",
    "os.makedirs(output_dir + \"/mv_show_images\", exist_ok=True)\n",
    "os.makedirs(output_dir + \"/videos\", exist_ok=True)\n",
    "os.makedirs(output_dir + \"/meshes\", exist_ok=True)\n",
    "os.makedirs(output_dir + \"/images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate \n",
    "samples = []\n",
    "\n",
    "for poke_sample in view:\n",
    "\n",
    "    # Set Inputs\n",
    "    image_path = open(poke_sample.filepath, \"rb\")\n",
    "    inputs = {\n",
    "        \"image_path\": image_path,\n",
    "        \"export_texmap\": True\n",
    "    }\n",
    "\n",
    "    # Run the Replicate Model\n",
    "    output = replicate.run(\n",
    "        \"camenduru/instantmesh:4f151757fd04d508b84f2192a17f58d11673971f05d9cb1fd8bd8149c6fc7cbb\",\n",
    "        input=inputs\n",
    "    )\n",
    "    \n",
    "    group = fo.Group()\n",
    "\n",
    "    # First create our input sample\n",
    "    sample = fo.Sample(filepath=poke_sample.filepath, group=group.element(\"Pokemon Picture\"))\n",
    "    samples.append(sample)\n",
    "\n",
    "    # Download and add the MV image sample\n",
    "    download_path = output_dir + \"/mv_show_images/\" + poke_sample.id + \".png\"\n",
    "    download_image(output[0], download_path)\n",
    "    sample = fo.Sample(filepath=download_path, group=group.element(\"MV Show Image\"))\n",
    "    samples.append(sample)\n",
    "\n",
    "    # Download and add the 3D video\n",
    "    download_path = output_dir + \"/videos/\" + output[1].split(\"/\")[-1]\n",
    "    download_image(output[1], download_path)\n",
    "    sample = fo.Sample(filepath=download_path, group=group.element(\"Video\"))\n",
    "    samples.append(sample)\n",
    "\n",
    "    # Download and add the mesh files\n",
    "    download_path_obj = output_dir + \"/meshes/\" + output[2].split(\"/\")[-1]\n",
    "    download_image(output[2], download_path_obj)\n",
    "    \n",
    "    download_path_mtl = output_dir + \"/meshes/\" + output[3].split(\"/\")[-1]\n",
    "    download_image(output[3], download_path_mtl)\n",
    "    \n",
    "    download_path_image = output_dir + \"/meshes/\" + output[4].split(\"/\")[-1]\n",
    "    download_image(output[4], download_path_image)\n",
    "    \n",
    "    obj = fo.ObjMesh(name=poke_sample[\"ground_truth\"].label, obj_path=download_path_obj, mtl_path=download_path_mtl)\n",
    "\n",
    "    # Create the Scene Sample\n",
    "    scene = fo.Scene(camera=fo.PerspectiveCamera(up=\"Z\"))\n",
    "    scene.add(obj)   \n",
    "    scene_path = output_dir + \"/meshes/\" + poke_sample.id + \".fo3d\"\n",
    "    scene.write(scene_path)\n",
    "    \n",
    "    sample = fo.Sample(scene_path, group=group.element(\"Mesh\"))\n",
    "    samples.append(sample)\n",
    "\n",
    "mesh_dataset.add_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30dc6e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session.dataset = mesh_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52b1af-4dce-428c-a0d5-5d460d385be9",
   "metadata": {},
   "source": [
    "Let's check our Charmander!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2c16f",
   "metadata": {},
   "source": [
    "![charmander](../assets/charmander.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60268b",
   "metadata": {},
   "source": [
    "## MVS Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d61d7e",
   "metadata": {},
   "source": [
    "The other most popular form of 3D Reconstruction is Multi-View Stereo Models. These models unlike the previous one take in multiple different angles to produce a sharp and accurate 3D Model. The data going in can vary from just a few images with camera intrinsics or full videos. After some preprocessing, the model takes the input and is able to generate the full 3D mesh. MVS models tend to perform better on tasks that require creating a 3D mesh for an entire room or building exterior. Let's take a look on how to generate a mesh using [simplerecon](https://github.com/nianticlabs/simplerecon) on scene data they have provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nianticlabs/simplerecon.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f1f866",
   "metadata": {},
   "source": [
    "We need to download the demo dataset as well as the model checkpoint in order to inference. We will use the [eta](https://github.com/voxel51/eta) library to handle both of these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a7dc6-a8f5-48f0-984b-d7906218b528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pytorch-lightning 1.5.4 has a non-standard dependency specifier torch>=1.7.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install kornia==0.6.7 antialiased-cnns efficientnet_pytorch timm trimesh transforms3d einops moviepy \\\n",
    "pyrender    scipy  setuptools==59.5.0 open3d pytorch-lightning==1.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa15c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    }
   ],
   "source": [
    "import eta.core.web as etaw\n",
    "import eta.core.utils as etau\n",
    "\n",
    "data_path = \"vdr.zip\"\n",
    "etaw.download_google_drive_file(\"1x-auV7vGCMdu5yZUMPcoP83p77QOuasT\", path=data_path)\n",
    "etau.extract_zip(data_path, delete_zip=True)\n",
    "\n",
    "ckpt_path = \"./simplerecon/weights/hero_model.ckpt\"\n",
    "etaw.download_google_drive_file(\"1hCuKZjEq-AghrYAmFxJs_4eeixIlP488\", path=ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f3c5f",
   "metadata": {},
   "source": [
    "Start by loading in our demo dataset, vdr. Since the dataset format is custom, we will load it in ourselves manually. Luckily, we can handle custom data ingestion easily with just the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2efda4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import os \n",
    "\n",
    "vdr_dataset = fo.Dataset(\"vdr\", overwrite=True)\n",
    "\n",
    "scans = os.listdir(\"./vdr/scans\")\n",
    "\n",
    "samples = []\n",
    "for scan in scans:\n",
    "    scan_list = os.listdir(\"./vdr/scans/\" + scans[0])\n",
    "    scan_list.sort()\n",
    "    frame_files = [file for file in scan_list if file.startswith(\"frame\")]\n",
    "    \n",
    "    for frame in frame_files:\n",
    "        sample = fo.Sample(\"./vdr/scans/\" + scan + \"/\" + frame)\n",
    "        sample[\"frame\"] = int(frame.split(\"_\")[1].split(\".\")[0])\n",
    "        sample[\"scan\"] = scan\n",
    "        samples.append(sample)\n",
    "        \n",
    "vdr_dataset.add_samples(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962f8d9",
   "metadata": {},
   "source": [
    "We can visualize our samples within the app. Check out [dynamic groups](https://docs.voxel51.com/user_guide/using_views.html#view-groups) by grouping our dataset by scan and ordering by frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5b3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdr_dataset.persistent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d075a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(vdr_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0c171-7a46-4bec-9df1-c4bbfd06708e",
   "metadata": {},
   "source": [
    "![vdr](../assets/vdr.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72268470",
   "metadata": {},
   "source": [
    "To begin inference, we will use simplerecons test.py to generate the meshes for the demo dataset. We use python subprocess again to call the model for us with the apropriate input variables. The command will generate meshes for both scans in our demo dataset and output them to our specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9581fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "output_dir = \"/path/to/simplerecon_outputs\"\n",
    "\n",
    "command = \"cd ./simplerecon && python test.py --name HERO_MODEL \\\n",
    "            --output_base_path \" + output_dir + \" \\\n",
    "            --config_file configs/models/hero_model.yaml \\\n",
    "            --load_weights_from_checkpoint weights/hero_model.ckpt \\\n",
    "            --dataset_path ../vdr \\\n",
    "            --tuple_info_file_location data_splits/vdr/  \\\n",
    "            --dataset_scan_split_file data_splits/vdr/scans.txt     \\\n",
    "            --dataset vdr           \\\n",
    "            --mv_tuple_file_suffix _eight_view_deepvmvs_dense.txt    \\\n",
    "            --num_images_in_tuple 8           \\\n",
    "            --frame_tuple_type dense            \\\n",
    "            --split test             \\\n",
    "            --num_workers 8 \\\n",
    "            --batch_size 2 \\\n",
    "            --fast_cost_volume \\\n",
    "            --run_fusion \\\n",
    "            --depth_fuser open3d \\\n",
    "            --fuse_color \\\n",
    "            --dump_depth_visualization \\\n",
    "            --cache_depths \"\n",
    "    \n",
    "result = subprocess.run(command, shell=True,capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbc97d-4ec5-490c-b95e-133dd9ca5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f94d4",
   "metadata": {},
   "source": [
    "Our model will output two key predictions, the mesh as we mentioned but also the predicted depth maps, which are used to generate the 3D meshes. We can visualize all of our depth maps easily by adding them to our VDR dataset using [fo.Heatmaps()](https://docs.voxel51.com/user_guide/using_datasets.html#heatmaps)!. Afterwards we see all of our depth maps in the app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3068cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_path = output_dir + \"/HERO_MODEL/vdr/dense/viz/quick_viz/\"\n",
    "\n",
    "for sample in vdr_dataset:\n",
    "    \n",
    "    if sample[\"frame\"] > 0: # no depth for first frame\n",
    "        color_path = viz_path + sample[\"scan\"] + \"/\" + str(sample[\"frame\"]) + \"_color.png\"\n",
    "        sample[\"color\"] = fo.Heatmap(map_path=color_path)\n",
    "\n",
    "        gt_depth_path = viz_path + sample[\"scan\"] + \"/\" + str(sample[\"frame\"]) + \"_gt_depth.png\"\n",
    "        sample[\"gt_depth\"] = fo.Heatmap(map_path=gt_depth_path)\n",
    "\n",
    "        pred_path = viz_path + sample[\"scan\"] + \"/\" + str(sample[\"frame\"]) + \"_pred_depth.png\"\n",
    "        sample[\"pred_depth\"] = fo.Heatmap(map_path=pred_path)\n",
    "\n",
    "        lowest_path = viz_path + sample[\"scan\"] + \"/\" + str(sample[\"frame\"]) + \"_lowest_cost_pred.png\"\n",
    "        sample[\"lowest_cost_pred\"] = fo.Heatmap(map_path=lowest_path)\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(vdr_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fdf966-2bc2-47b7-a026-ca00d0fe0856",
   "metadata": {},
   "source": [
    "We can [dynamically group](https://docs.voxel51.com/user_guide/using_views.html#view-groups) our samples and view all the different depth maps on our frames as a video!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba3363",
   "metadata": {},
   "source": [
    "![depth_prediction](../assets/depth_predictions.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417f13e",
   "metadata": {},
   "source": [
    "Similar to Instant Mesh, we can use a script in simplerecon that will allow us the generate a video summarizing all the diffent depth maps to understand how our model perceived its enviroment. To create the video, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c177367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/path/to/simplerecon_outputs\"\n",
    "\n",
    "command = \"\"\"cd ./simplerecon && python ./visualization_scripts/visualize_scene_depth_output.py \\\n",
    "            --name HERO_MODEL   \\\n",
    "            --output_base_path \"\"\" + output_dir + \"\"\"    \\\n",
    "            --dataset_path  ../vdr \\\n",
    "            --tuple_info_file_location data_splits/vdr/  \\\n",
    "            --dataset_scan_split_file data_splits/vdr/scans.txt     \\\n",
    "            --dataset vdr           \\\n",
    "            --mv_tuple_file_suffix _eight_view_deepvmvs_dense.txt    \\\n",
    "            --num_images_in_tuple 8           \\\n",
    "            --frame_tuple_type dense            \\\n",
    "            --split test             \\\n",
    "            --num_workers 8 \n",
    "            \"\"\"\n",
    "    \n",
    "result = subprocess.run(command, shell=True,capture_output=True, text=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1bd2c",
   "metadata": {},
   "source": [
    "It would also be handy if we created a plain video to compare to using ffmpeg to generate the video from the original frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b505436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -framerate 24 -i ./vdr/scans/house/frame_%d.jpg  -c:v libx264 ./vdr/scans/house/house.mp4\n",
    "!ffmpeg -framerate 24 -i ./vdr/scans/living_room/frame_%d.jpg  -c:v libx264 ./vdr/scans/living_room/living_room.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae3833",
   "metadata": {},
   "source": [
    "Finally, we can create a grouped dataset for our MVS results and add our original video, depth video, and our mesh for each scan in our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e579a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdr_mesh_dataset = fo.Dataset(\"vdr-meshes\", overwrite=True)\n",
    "vdr_mesh_dataset.add_group_field(\"group\", default=\"Original Video\")\n",
    "\n",
    "\n",
    "scans = os.listdir(\"./vdr/scans\")\n",
    "\n",
    "samples = []\n",
    "for scan in scans:\n",
    "    group = fo.Group()\n",
    "    sample = fo.Sample(\"./vdr/scans/\" + scan + \"/\" +scan + \".mp4\",group=group.element(\"Original Video\"))\n",
    "    sample[\"scan\"] = scan\n",
    "    samples.append(sample)\n",
    "    \n",
    "    depth_video = output_dir + \"/HERO_MODEL/vdr/dense/viz/depth_videos/\" + scan + \".mp4\"\n",
    "    sample = fo.Sample(depth_video,group=group.element(\"Depth Video\"))\n",
    "    samples.append(sample)\n",
    "    \n",
    "    scan_mesh = output_dir + \"/HERO_MODEL/vdr/dense/meshes/0.04_3.0_open3d_color/\" + scan + \".ply\"\n",
    "    ply = fo.PlyMesh(name=scan, ply_path=scan_mesh)\n",
    "    \n",
    "    scene = fo.Scene(camera=fo.PerspectiveCamera(up=\"Z\"))\n",
    "    scene.add(ply)   \n",
    "    scene_path = output_dir + \"/HERO_MODEL/vdr/dense/meshes/0.04_3.0_open3d_color/\" + scan + \".fo3d\"\n",
    "    scene.write(scene_path)\n",
    "    \n",
    "    sample = fo.Sample(scene_path, group=group.element(\"Mesh\"))\n",
    "    samples.append(sample)\n",
    "    \n",
    "    \n",
    "        \n",
    "vdr_mesh_dataset.add_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3650318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdr_mesh_dataset.persistent = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b56a4e",
   "metadata": {},
   "source": [
    "Our last step is to visualize our new dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(vdr_mesh_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93e5ec-0974-461f-be05-dbf77260603a",
   "metadata": {},
   "source": [
    "![vdr_mesh](../assets/vdr_mesh.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
